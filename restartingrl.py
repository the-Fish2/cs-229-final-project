# -*- coding: utf-8 -*-
"""RestartingRL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lbs1vhJtXJ-8IPIDeU7A8dqN2K5TUdUa
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt

NUM_SAMPLES = 1000

class FunctionInverseEnv:
    def __init__(self):
        self.a_space = [1, 2, 3]
        self.b_space = [1, 3, 5]
        self.c_space = [1, 2, 3]

        # Generate x values once
        self.x_values = np.linspace(-10, 10, NUM_SAMPLES)

        self.reset()

    def reset(self):
        """Reset environment with random function parameters"""
        self.a = random.choice(self.a_space)
        self.b = random.choice(self.b_space)
        self.c = random.choice(self.c_space)

        # Generate y values for the function y = ax^b + c
        self.current_y = self.a * np.power(np.abs(self.x_values), self.b) * np.sign(self.x_values)**self.b + self.c

        self.step_count = 0
        self.max_steps = 3  # subtract, divide, root
        self.done = False

        return self._get_state()

    def _get_state(self):
        """
        State representation: statistical features of current y values
        This helps the network understand the current transformation state
        """
        # Use statistics that are informative about the function
        features = np.array(self.current_y,
            # np.mean(self.current_y),
            # np.std(self.current_y),
            # np.min(self.current_y),
            # np.max(self.current_y),
            # np.median(self.current_y),
            # # Ratio features to help identify scaling
            # np.percentile(self.current_y, 75),
            # np.percentile(self.current_y, 25),
            # # How far from y=x (target)
            # np.mean(np.abs(self.current_y - self.x_values)),
            # self.step_count / self.max_steps  # Progress indicator
        dtype=np.float32)

        return features

    def regen_y_samples(self):
        """Generate new y values for the function"""
        self.current_y = self.a * np.power(np.abs(self.x_values), self.b) * np.sign(self.x_values)**self.b + self.c

    def step(self, action):
        """
        Execute action and return next state, reward, done
        Action encoding:
        0-2: subtract 1, 2, 3
        3-5: divide by 1, 2, 3
        6-8: root 1, 3, 5
        """
        if self.done:
            return self._get_state(), 0, True, {}

        action_type = action // 3  # 0: subtract, 1: divide, 2: root
        action_value_idx = action % 3

        # Map action values
        if action_type == 0:  # subtract
            value = [1, 2, 3][action_value_idx]

        elif action_type == 1:  # divide
            value = [1, 2, 3][action_value_idx]
            if value != 0:

        elif action_type == 2:  # root
            value = [1, 3, 5][action_value_idx]
            # Handle roots carefully
            if value == 1:
                pass  # y^(1/1) = y
            else:
                # For odd roots, preserve sign
                self.current_y = np.sign(self.current_y) * np.power(np.abs(self.current_y), 1.0/value)

        self.step_count += 1

        # Calculate reward
        reward = self._calculate_reward()

        # Check if done
        if self.step_count >= self.max_steps:
            self.done = True

        info = {
            'true_params': (self.a, self.b, self.c),
            'action_type': action_type,
            'action_value': value if 'value' in locals() else None
        }

        return self._get_state(), reward, self.done, info

    def _calculate_reward(self):
        """
        Calculate reward based on how close current_y is to x
        """
        # Mean absolute error from y = x
        mae = np.mean(np.abs(self.current_y - self.x_values))

        # Mean squared error
        mse = np.mean((self.current_y - self.x_values)**2)

        # If very close to y = x, give large reward
        if mae < 0.1:
            return 100.0
        elif mae < 1.0:
            return 10.0
        elif mae < 10.0:
            return 1.0
        else:
            # Penalize based on error
            return -mae / 100.0

class DQN(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=128):
        super(DQN, self).__init__()

        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x

class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones)
        )

    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma

        # Epsilon-greedy parameters
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        # Q-Networks
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy_net = DQN(state_size, action_size).to(self.device)
        self.target_net = DQN(state_size, action_size).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        # Optimizer
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)

        # Replay buffer
        self.memory = ReplayBuffer(capacity=10000)

        self.batch_size = 64
        self.update_target_every = 10
        self.updates = 0

    def select_action(self, state, training=True):
        """Select action using epsilon-greedy policy"""
        if training and random.random() < self.epsilon:
            return random.randrange(self.action_size)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.policy_net(state_tensor)
            return q_values.argmax().item()

    def train_step(self):
        """Perform one training step"""
        if len(self.memory) < self.batch_size:
            return None

        # Sample batch
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)

        # Convert to tensors
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)

        # Current Q values
        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))

        # Next Q values from target network
        with torch.no_grad():
            next_q_values = self.target_net(next_states).max(1)[0]
            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values

        # Compute loss
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)

        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()

        # Update target network
        self.updates += 1
        if self.updates % self.update_target_every == 0:
            self.target_net.load_state_dict(self.policy_net.state_dict())

        return loss.item()

    def update_epsilon(self):
        """Decay epsilon"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

def train_agent(num_episodes=1000, max_steps_per_episode=3):
    env = FunctionInverseEnv()
    state_size = NUM_SAMPLES  # From _get_state()
    action_size = 9  # 3 operations Ã— 3 values each

    agent = DQNAgent(state_size, action_size)
    print_arr = ["add 1", "add 2", "add 3", "mul 1", "mul 2", "mul 3", "f(x)=x", "f(x)=x^3", "f(x)=x^5"]
    # Tracking metrics
    episode_rewards = []
    episode_losses = []
    success_rate = []
    recent_successes = deque(maxlen=100)

    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        episode_loss = []

        for step in range(max_steps_per_episode):
            # Select and perform action
            action = agent.select_action(state, training=True)
            next_state, reward, done, info = env.step(action)

            print(print_arr[action], end=" ")

            # Store transition
            agent.memory.push(state, action, reward, next_state, done)

            # Train
            loss = agent.train_step()
            if loss is not None:
                episode_loss.append(loss)

            episode_reward += reward
            state = next_state

            if done:
                break

        print(f" ")

        # Update epsilon
        agent.update_epsilon()

        # Track metrics
        episode_rewards.append(episode_reward)
        if episode_loss:
            episode_losses.append(np.mean(episode_loss))

        # Check success (reward >= 10 means close to y=x)
        success = episode_reward >= 10
        recent_successes.append(success)
        success_rate.append(np.mean(recent_successes))

        # Logging
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_loss = np.mean(episode_losses[-100:]) if episode_losses else 0
            current_success_rate = np.mean(recent_successes)

            print(f"Episode {episode + 1}/{num_episodes}")
            print(f"  Avg Reward: {avg_reward:.2f}")
            print(f"  Avg Loss: {avg_loss:.4f}")
            print(f"  Success Rate: {current_success_rate:.2%}")
            print(f"  Epsilon: {agent.epsilon:.3f}")
            print(f"  Last episode params: a={env.a}, b={env.b}, c={env.c}")
            print()

    return agent, episode_rewards, episode_losses, success_rate

def evaluate_agent(agent, num_episodes=100):
    env = FunctionInverseEnv()
    successes = 0
    action_sequences = []

    for episode in range(num_episodes):
        state = env.reset()
        actions_taken = []
        true_params = (env.a, env.b, env.c)

        for step in range(3):
            action = agent.select_action(state, training=False)
            next_state, reward, done, info = env.step(action)
            actions_taken.append((info['action_type'], info['action_value']))
            state = next_state

            if done:
                if reward >= 10:
                    successes += 1
                break

        action_sequences.append({
            'params': true_params,
            'actions': actions_taken,
            'final_reward': reward
        })

    print(f"\nEvaluation Results ({num_episodes} episodes):")
    print(f"Success Rate: {successes/num_episodes:.2%}")
    print("\nSample action sequences:")
    for i in range(min(5, len(action_sequences))):
        seq = action_sequences[i]
        print(f"  Params (a={seq['params'][0]}, b={seq['params'][1]}, c={seq['params'][2]})")
        print(f"  Actions: {seq['actions']}")
        print(f"  Reward: {seq['final_reward']:.2f}")
        print()

    return successes / num_episodes, action_sequences

def plot_training_results(episode_rewards, episode_losses, success_rate):
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # Plot rewards
    axes[0].plot(episode_rewards, alpha=0.3, label='Episode Reward')
    axes[0].plot(np.convolve(episode_rewards, np.ones(100)/100, mode='valid'),
                 label='Moving Average (100)')
    axes[0].set_xlabel('Episode')
    axes[0].set_ylabel('Reward')
    axes[0].set_title('Training Rewards')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot losses
    if episode_losses:
        axes[1].plot(episode_losses, alpha=0.3, label='Episode Loss')
        axes[1].plot(np.convolve(episode_losses, np.ones(100)/100, mode='valid'),
                     label='Moving Average (100)')
        axes[1].set_xlabel('Episode')
        axes[1].set_ylabel('Loss')
        axes[1].set_title('Training Loss')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)

    # Plot success rate
    axes[2].plot(success_rate)
    axes[2].set_xlabel('Episode')
    axes[2].set_ylabel('Success Rate')
    axes[2].set_title('Success Rate (100-episode window)')
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_results.png', dpi=150, bbox_inches='tight')
    plt.show()

def test_specific_case(agent, a, b, c):
    """Test agent on a specific function configuration"""
    env = FunctionInverseEnv()

    # Manually set parameters
    env.a = a
    env.b = b
    env.c = c
    env.current_y = env.a * np.power(np.abs(env.x_values), env.b) * np.sign(env.x_values)**env.b + env.c
    env.step_count = 0
    env.done = False

    state = env._get_state()

    print(f"\nTesting case: a={a}, b={b}, c={c}")
    print(f"Ground truth sequence: subtract {c}, divide by {a}, root {b}")
    print(f"Initial y range: [{env.current_y.min():.2f}, {env.current_y.max():.2f}]")
    print()

    actions_taken = []

    for step in range(3):
        action = agent.select_action(state, training=False)
        next_state, reward, done, info = env.step(action)

        action_type_name = ['subtract', 'divide', 'root'][info['action_type']]
        actions_taken.append((action_type_name, info['action_value']))

        print(f"Step {step + 1}: {action_type_name} {info['action_value']}")
        print(f"  y range: [{env.current_y.min():.2f}, {env.current_y.max():.2f}]")
        print(f"  Reward: {reward:.2f}")

        state = next_state

        if done:
            break

    # Check final result
    final_error = np.mean(np.abs(env.current_y - env.x_values))
    print(f"\nFinal MAE from y=x: {final_error:.4f}")
    print(f"Success: {final_error < 0.1}")

    # Visualize
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # Plot original function
    original_y = a * np.power(np.abs(env.x_values), b) * np.sign(env.x_values)**b + c
    axes[0].plot(env.x_values, original_y, label=f'y = {a}x^{b} + {c}', linewidth=2)
    axes[0].plot(env.x_values, env.x_values, '--', label='y = x (target)', alpha=0.5)
    axes[0].set_xlabel('x')
    axes[0].set_ylabel('y')
    axes[0].set_title('Original Function')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot final result
    axes[1].plot(env.x_values, env.current_y, label='Final y', linewidth=2)
    axes[1].plot(env.x_values, env.x_values, '--', label='y = x (target)', alpha=0.5)
    axes[1].set_xlabel('x')
    axes[1].set_ylabel('y')
    axes[1].set_title(f'After transformations\nMAE: {final_error:.4f}')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'test_case_a{a}_b{b}_c{c}.png', dpi=150, bbox_inches='tight')
    plt.show()

    return actions_taken, final_error

print("=" * 60)
print("RL Model for Function Parameter Identification")
print("=" * 60)
print("\nProblem: Given y = ax^b + c, learn to identify a, b, c")
print("Actions: subtract {1,2,3}, divide by {1,2,3}, root {1,3,5}")
print("Goal: Transform y to match x\n")

# Train the agent
print("Starting training...")
agent, episode_rewards, episode_losses, success_rate = train_agent(
    num_episodes=1000,
    max_steps_per_episode=3
)

# Plot training results
print("\nPlotting training results...")
plot_training_results(episode_rewards, episode_losses, success_rate)

# Evaluate the trained agent
print("\nEvaluating trained agent...")
eval_success_rate, action_sequences = evaluate_agent(agent, num_episodes=100)

# Test on all possible combinations
print("\n" + "=" * 60)
print("Testing on all parameter combinations")
print("=" * 60)

a_values = [1, 2, 3]
b_values = [1, 3, 5]
c_values = [1, 2, 3]

all_results = []

for a in a_values:
    for b in b_values:
        for c in c_values:
            actions, error = test_specific_case(agent, a, b, c)
            all_results.append({
                'params': (a, b, c),
                'actions': actions,
                'error': error,
                'success': error < 0.1
            })

# Summary statistics
print("\n" + "=" * 60)
print("FINAL RESULTS SUMMARY")
print("=" * 60)

total_cases = len(all_results)
successful_cases = sum(1 for r in all_results if r['success'])

print(f"\nTotal test cases: {total_cases}")
print(f"Successful cases: {successful_cases}")
print(f"Success rate: {successful_cases/total_cases:.2%}")

print("\nFailed cases:")
for result in all_results:
    if not result['success']:
        a, b, c = result['params']
        print(f"  a={a}, b={b}, c={c}: MAE={result['error']:.4f}")
        print(f"    Actions: {result['actions']}")

# Save the trained model
print("\nSaving trained model...")
torch.save({
    'policy_net_state_dict': agent.policy_net.state_dict(),
    'target_net_state_dict': agent.target_net.state_dict(),
    'optimizer_state_dict': agent.optimizer.state_dict(),
    'episode_rewards': episode_rewards,
    'success_rate': success_rate,
}, 'trained_agent.pth')
print("Model saved to 'trained_agent.pth'")

print("\n" + "=" * 60)
print("Training and evaluation complete!")
print("=" * 60)

